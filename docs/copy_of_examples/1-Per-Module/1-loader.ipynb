{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loader\n",
    "\n",
    "This is where your urban data journey begins. Whether youâ€™ve got `CSV`, `Parquet`, `Shapefiles`, or want to use `HuggingFace` datasets weâ€™ll get them loaded up and ready to explore. UrbanMapper provides two main ways to load data:\n",
    "\n",
    "1. **Manual Loading of Local Datasets**: You can load datasets available locally in various formats like `CSV`, `Parquet`, and `Shapefiles`. This is the default approach for working with your own data.\n",
    "2. **Integration with Hugging Face Dataset Library**: UrbanMapper also supports loading datasets from the Hugging Face library via the `from_dataframe()` method. This broadens the possibilities for integrating external data sources seamlessly.\n",
    "\n",
    "**Data source used**:\n",
    "- PLUTO data from NYC Open Data. https://www.nyc.gov/content/planning/pages/resources/datasets/mappluto-pluto-change\n",
    "- Taxi data from NYC Open Data. https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "\n",
    "- **The OSCUR Hugging Face Dataset Source:**\n",
    "The [OSCUR Hugging Face organization](https://huggingface.co/oscur)\n",
    " hosts all datasets associated with [OSCUR](https://oscur.org/): Open-Source Cyberinfrastructure for Urban Computing, a research initiative focused on enabling reproducible, scalable, and accessible data-driven analysis for urban environments.\n",
    "By using the OSCUR datasets, you can skip downloading datasets from Google Drive or official links locally. These datasets are ready to use in all subsequent notebook examples without issue, making your workflow more efficient and seamless.\n",
    "\n",
    "\n",
    "\n",
    "Ready? Letâ€™s dive in! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urban_mapper as um\n",
    "\n",
    "# Start up UrbanMapper\n",
    "mapper = um.UrbanMapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CSV Data\n",
    "\n",
    "First up, letâ€™s load a CSV file with PLUTO data. Weâ€™ll tell UrbanMapper where to find the longitude and latitude columns so it knows whatâ€™s what and can make sure those colums are well formatted prior any analysis.\n",
    "\n",
    "Note that below we employ a given csv, but you can put your own path, try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_loader = (\n",
    "    mapper\n",
    "    .loader # From the loader module\n",
    "    .from_file(\"<path_to>/pluto.csv\") # To update with your own path\n",
    "    .with_columns(longitude_column=\"longitude\", latitude_column=\"latitude\") # Inform your long and lat columns\n",
    ")\n",
    "\n",
    "gdf = csv_loader.load() # Load the data and create a geodataframe's instance\n",
    "\n",
    "# gdf stands for GeoDataFrame, like df in pandas for dataframes.\n",
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Parquet Data\n",
    "\n",
    "Next, let's grab a `parquet` based dataset for the example. Same workflow as for the csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_loader = (\n",
    "    mapper.\n",
    "    loader. # From the loader module\n",
    "    from_file(\"<path_to>/taxisvis5M.parquet\") # To update with your own path\n",
    "    .with_columns(\"pickup_longitude\", \"pickup_latitude\") # Inform your long and lat columns\n",
    ")\n",
    "\n",
    "gdf = parquet_loader.load() # Load the data and create a geodataframe's instance\n",
    "\n",
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Shapefile Data\n",
    "\n",
    "Finally, letâ€™s load a Shapefile-based dataset. Shapefiles have geometry built in, so no need to specify columns â€” UrbanMapper sorts it out for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_loader = (\n",
    "    mapper\n",
    "    .loader # From the loader module\n",
    "    .from_file(\"<path_to>/MapPLUTO.shp\") # To update with your own path\n",
    ")\n",
    "\n",
    "gdf = shp_loader.load() # Load the data and create a geodataframe's instance\n",
    "\n",
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data from Hugging Face\n",
    "\n",
    "UrbanMapper provides two ways to load datasets from Hugging Face:\n",
    "\n",
    "1. **Using `from_dataframe()`**: This method allows you to load a dataset into a pandas DataFrame first, giving you flexibility to preprocess or explore the data before loading it into UrbanMapper.\n",
    "2. **Using `from_huggingface()`**: This method directly loads the dataset into UrbanMapper, skipping the intermediate DataFrame step for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Using `from_dataframe()`\n",
    "\n",
    "This code loads the \"oscur/pluto\" dataset from Hugging Face, selects the training split, and converts the first 1,000 rows into a pandas DataFrame for efficient analysis and exploration. The resulting DataFrame can then be loaded into UrbanMapper using `from_dataframe()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Retrieve the dataset from Hugging Face\n",
    "dataset = load_dataset(\"oscur/pluto\")\n",
    "# Select the training split\n",
    "train_ds = dataset[\"train\"]\n",
    "# Convert the first 1000 rows to a DataFrame\n",
    "df = pd.DataFrame(train_ds[:1000])\n",
    "\n",
    "# Load the dataset using UrbanMapper\n",
    "df_loader = (\n",
    "    mapper\n",
    "    .loader # From the loader module\n",
    "    .from_dataframe(df) # To update with your dataframe\n",
    "    .with_columns(longitude_column=\"longitude\", latitude_column=\"latitude\") # Inform your long and lat columns\n",
    ")\n",
    "\n",
    "gdf = df_loader.load() # Load the data and create a geodataframe's instance\n",
    "\n",
    "# gdf stands for GeoDataFrame, like df in pandas for dataframes.\n",
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Using `from_huggingface()`\n",
    "\n",
    "This method directly loads the \"oscur/pluto\" dataset into UrbanMapper, skipping the intermediate DataFrame step. It's a simpler and faster way to load datasets hosted on Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a full dataset directly from Hugging Face\n",
    "loader = mapper.loader.from_huggingface(\"oscur/pluto\", number_of_rows=100).with_columns(longitude_column=\"longitude\", latitude_column=\"latitude\")\n",
    "gdf = loader.load()\n",
    "gdf  # Next steps: analyze or visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Be Able To Preview Your Loader's instance\n",
    "\n",
    "Additionally, you can preview your loader's instance to see what columns you've specified and the file path you've loaded from. Pretty useful when you load a urban analysis shared by someone else and might want to check what columns are being used for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdf.preview())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading many datasets to feed and end-to-end UrbanMapper process (step-by-step or pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets directly from Hugging Face\n",
    "pluto_data = mapper.loader.from_huggingface(\"oscur/pluto\", number_of_rows=100).with_columns(longitude_column=\"longitude\", latitude_column=\"latitude\").load()\n",
    "taxi_data =  (\n",
    "  mapper\n",
    "  .loader\n",
    "  .from_huggingface(\"oscur/taxisvis1M\", number_of_rows=100)\n",
    "  .with_columns(longitude_column=\"pickup_longitude\", latitude_column=\"pickup_latitude\")\n",
    "  .with_map({\"pickup_longitude\": \"longitude\", \"pickup_latitude\": \"latitude\"}) ## Routines like layer.map_nearest_layer needs datasets with the same longitude_column and latitude_column\n",
    "  .load()\n",
    ")\n",
    "## ... load any other dataset\n",
    "\n",
    "data = {\n",
    "  \"pluto_data\": pluto_data,\n",
    "  \"taxi_data\": taxi_data,\n",
    "  ## ... add any other dataset\n",
    "}\n",
    "\n",
    "## Invoke any other UrbanMapper module passing data as parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping Up\n",
    "\n",
    "And thatâ€™s that! ðŸŽˆ Youâ€™ve loaded data from four different formats like a pro: `CSV`, `Parquet`, `Shapefile`, and datasets from Hugging Face. Now youâ€™re all set to play with modules like `urban_layer` or `imputer`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
